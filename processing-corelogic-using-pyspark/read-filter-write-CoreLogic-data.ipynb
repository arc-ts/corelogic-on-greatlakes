{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d5aec",
   "metadata": {},
   "source": [
    "# Processing the CoreLogic Data using PySpark\n",
    "\n",
    "In this Jupyter notebook, we demonstrate how you can process the CoreLogic data using PySpark. \n",
    "\n",
    "In particular, we will show you how to:\n",
    "* import the data, \n",
    "* explore rows and columns, \n",
    "* filter for a given location, \n",
    "* inspect the data for missing and erroneous information, \n",
    "* correct data errors,\n",
    "* and write a subset of the data to a csv file for later use. \n",
    "\n",
    "This [PySpark cheat sheet](http://datacamp-community-prod.s3.amazonaws.com/acfa4325-1d43-4542-8ce4-bea2d287db10) provides a great overview of available PySpark functionality. \n",
    "\n",
    "The notebook was created using a 'Jupyter + Spark Basic session' in [Open on Demand](https://arc.umich.edu/open-ondemand/) (OOD) on [Great Lakes](https://arc.umich.edu/greatlakes/) (GL). This automatically initializes Spark in the background.\n",
    "\n",
    "If you are not running the notebook using OOD on GL, you will most likely have to make sure PySpark is installed on your system and then initialize it by typing \n",
    "\n",
    "```from pyspark import SparkContext```\n",
    "\n",
    "```sc = SparkContext(master = 'local[2]')```\n",
    "\n",
    "If you encounter any errors or have questions about this Jupyter notebook, or if you would like us to add another PySpark example using the CoreLogic data, feel free to reach out to [Armand Burks](mailto:arburks@umich.edu) and [Jule Kr√ºger](mailto:julianek@umich.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc16a3e",
   "metadata": {},
   "source": [
    "## Importing the CoreLogic data with PySpark\n",
    "\n",
    "The CoreLogic data are stored in a Turbo volume on the ```/nfs``` drive. To get access, you will need to sign a Memorandum of Understanding (MOU) with the University of Michigan Library. For more information about this, see [here](https://github.com/arc-ts/corelogic-on-greatlakes/tree/main/intro-to-corelogic-data). To execute this notebook, you will have to be granted access to the CoreLogic data on Turbo.\n",
    "\n",
    "The raw data come in three separate files: deeds (28GB), foreclosures (6GB), and taxes (24GB). [CSCAR](https://cscar.research.umich.edu/) pre-processed the CoreLogic data and stored each raw file in 100 separate partitions. You can read more about this methodology in ```nfs/turbo/lib-data-corelogic/Docs/cscar_data.txt```.\n",
    "\n",
    "We are going to work with the pre-processed data to improve import speeds. Let's store the paths to the pre-processed partitioned files in three separate variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce474a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo_path_fcl = \"/nfs/turbo/lib-data-corelogic/Data/fcl/*.gz\"\n",
    "turbo_path_deed = \"/nfs/turbo/lib-data-corelogic/Data/deed/*.gz\"\n",
    "turbo_path_tax = \"/nfs/turbo/lib-data-corelogic/Data/tax/*.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ba4b9",
   "metadata": {},
   "source": [
    "The following command imports a set of partitioned CoreLogic data files into a dataframe (df). You can switch between foreclosures, deeds, and taxes by choosing the relevant path accordingly. In each text file, the pipe ```|``` was used as a delimiter and is set  in the ```sep``` argument as such.\n",
    "\n",
    "__A note on import speeds__: Because of their size, it takes a little while to read in each partitioned data file collection. Importing the raw data files takes much longer. Using the pre-processed data from CSCAR, which distributes the data across 100 partitions, greatly improves import speeds as Spark can spread computation across multiple cores.\n",
    "\n",
    "Let's work with the foreclosure data for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5904b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(turbo_path_fcl, header=True, sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fac120",
   "metadata": {},
   "source": [
    "(A small note on reading in the data: You could set the ```inferSchema=True``` argument, which would make Python infer the column type. The drawback of this is that FIPS and ZIP codes that start with a `0` have the leading `0` removed in the creation of an integer variable. We choose to keep ```inferSchema=False```, the default argument, to avoid this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978f9af",
   "metadata": {},
   "source": [
    "## Exploring the CoreLogic data (rows and columns)\n",
    "\n",
    "First, we want to get a basic idea of the CoreLogic data. What are the columns and what types of values do they contain? The .printSchema() method prints the data type (integer, string, double) for each column in the dataset. \n",
    "\n",
    "Because we set ```inferSchema=False```, all columns are read in as string data types. ```nullable = true``` means that a given column can accept missing (```null```) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eebc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- APN: string (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- BatchDateSeq Number: string (nullable = true)\n",
      " |-- DeedCategory: string (nullable = true)\n",
      " |-- DocumentType: string (nullable = true)\n",
      " |-- RecordingDate: string (nullable = true)\n",
      " |-- DocumentYear: string (nullable = true)\n",
      " |-- DocumentNumber: string (nullable = true)\n",
      " |-- DocumentBook: string (nullable = true)\n",
      " |-- DocumentPage: string (nullable = true)\n",
      " |-- TitleCompanyCode: string (nullable = true)\n",
      " |-- TitleCompanyName: string (nullable = true)\n",
      " |-- AttorneyName: string (nullable = true)\n",
      " |-- AttorneyPhoneNumber: string (nullable = true)\n",
      " |-- 1stDefendantBorrowerOwnerFirstName: string (nullable = true)\n",
      " |-- 1stDefendantBorrowerOwnerLastName: string (nullable = true)\n",
      " |-- 1stDefendantBorrowerOwnerCompanyName: string (nullable = true)\n",
      " |-- 2ndDefendantBorrowerOwnerFirstName: string (nullable = true)\n",
      " |-- 2ndDefendantBorrowerOwnerLastName: string (nullable = true)\n",
      " |-- 2ndDefendantBorrowerOwnerCompanyName: string (nullable = true)\n",
      " |-- 3rdDefendantBorrowerOwnerFirstName: string (nullable = true)\n",
      " |-- 3rdDefendantBorrowerOwnerLastName: string (nullable = true)\n",
      " |-- 3rdDefendantBorrowerOwnerCompanyName: string (nullable = true)\n",
      " |-- 4thDefendantBorrowerOwnerFirstName: string (nullable = true)\n",
      " |-- 4thDefendantBorrowerOwnerLastName: string (nullable = true)\n",
      " |-- 4thDefendantBorrowerOwnerCompanyName: string (nullable = true)\n",
      " |-- DefendantBorrowerOwnerEtAlIndicator: string (nullable = true)\n",
      " |-- Filler1: string (nullable = true)\n",
      " |-- DateofDefault: string (nullable = true)\n",
      " |-- AmountofDefault: string (nullable = true)\n",
      " |-- Filler2: string (nullable = true)\n",
      " |-- FilingDate: string (nullable = true)\n",
      " |-- CourtCaseNumber: string (nullable = true)\n",
      " |-- LisPendensType: string (nullable = true)\n",
      " |-- Plaintiff1/Seller: string (nullable = true)\n",
      " |-- Plaintiff2/Seller: string (nullable = true)\n",
      " |-- FinalJudgmentAmount: string (nullable = true)\n",
      " |-- Filler3: string (nullable = true)\n",
      " |-- AuctionDate: string (nullable = true)\n",
      " |-- AuctionTime: string (nullable = true)\n",
      " |-- StreetAddressofAuctionCall: string (nullable = true)\n",
      " |-- CityofAuctionCall: string (nullable = true)\n",
      " |-- StateofAuctionCall: string (nullable = true)\n",
      " |-- OpeningBid: string (nullable = true)\n",
      " |-- Filler4: string (nullable = true)\n",
      " |-- SalesPrice: string (nullable = true)\n",
      " |-- SitusAddressIndicator1: string (nullable = true)\n",
      " |-- SitusHouseNumberPrefix1: string (nullable = true)\n",
      " |-- SitusHouseNumber1: string (nullable = true)\n",
      " |-- SitusHouseNumberSuffix1: string (nullable = true)\n",
      " |-- SitusStreetName1: string (nullable = true)\n",
      " |-- SitusMode1: string (nullable = true)\n",
      " |-- SitusDirection1: string (nullable = true)\n",
      " |-- SitusQuadrant1: string (nullable = true)\n",
      " |-- ApartmentUnit1: string (nullable = true)\n",
      " |-- PropertyCity1: string (nullable = true)\n",
      " |-- PropertyState1: string (nullable = true)\n",
      " |-- PropertyAddressZipCode1: string (nullable = true)\n",
      " |-- CarrierCode1: string (nullable = true)\n",
      " |-- FullSiteAddressUnparsed1: string (nullable = true)\n",
      " |-- LenderBeneficiaryFirstName: string (nullable = true)\n",
      " |-- LenderBeneficiaryLastName: string (nullable = true)\n",
      " |-- LenderBeneficiaryCompanyName: string (nullable = true)\n",
      " |-- LenderBeneficiaryMailingAddress: string (nullable = true)\n",
      " |-- LenderBeneficiaryCity: string (nullable = true)\n",
      " |-- LenderBeneficiaryState: string (nullable = true)\n",
      " |-- LenderBeneficiaryZip: string (nullable = true)\n",
      " |-- LenderPhone: string (nullable = true)\n",
      " |-- Filler5: string (nullable = true)\n",
      " |-- TrusteeName: string (nullable = true)\n",
      " |-- TrusteeMailing Address: string (nullable = true)\n",
      " |-- TrusteeCity: string (nullable = true)\n",
      " |-- TrusteeState: string (nullable = true)\n",
      " |-- TrusteeZip: string (nullable = true)\n",
      " |-- TrusteePhone: string (nullable = true)\n",
      " |-- TrusteesSaleNumber: string (nullable = true)\n",
      " |-- Filler6: string (nullable = true)\n",
      " |-- OriginalLoanDate: string (nullable = true)\n",
      " |-- OriginalLoanRecordingDate: string (nullable = true)\n",
      " |-- OriginalLoanAmount: string (nullable = true)\n",
      " |-- OriginalDocumentNumber: string (nullable = true)\n",
      " |-- OriginalRecordingBook: string (nullable = true)\n",
      " |-- OriginalRecordingPage: string (nullable = true)\n",
      " |-- Filler7: string (nullable = true)\n",
      " |-- ParcelNumberParcel ID: string (nullable = true)\n",
      " |-- ParcelNumberUnformattedID: string (nullable = true)\n",
      " |-- LastFullSaleTransferDate: string (nullable = true)\n",
      " |-- TransferValue: string (nullable = true)\n",
      " |-- Mailing AddressIndicator2: string (nullable = true)\n",
      " |-- Mailing HouseNumberPrefix2: string (nullable = true)\n",
      " |-- MailingHouseNumber2: string (nullable = true)\n",
      " |-- MailingHouseNumberSuffix2: string (nullable = true)\n",
      " |-- MailingStreetName2: string (nullable = true)\n",
      " |-- MailingMode2: string (nullable = true)\n",
      " |-- MailingDirection2: string (nullable = true)\n",
      " |-- MailingQuadrant2: string (nullable = true)\n",
      " |-- Mailing ApartmentUnit2: string (nullable = true)\n",
      " |-- Mailing PropertyCity2: string (nullable = true)\n",
      " |-- MailingPropertyState2: string (nullable = true)\n",
      " |-- Mailing PropertyAddressZipCode2: string (nullable = true)\n",
      " |-- Mailing CarrierCode2: string (nullable = true)\n",
      " |-- FullSiteAddressUnparsed2: string (nullable = true)\n",
      " |-- PropertyIndicator: string (nullable = true)\n",
      " |-- UseCode: string (nullable = true)\n",
      " |-- NumberofUnits: string (nullable = true)\n",
      " |-- LivingAreaSquareFeet: string (nullable = true)\n",
      " |-- NumberofBedrooms: string (nullable = true)\n",
      " |-- NumberofBathrooms: string (nullable = true)\n",
      " |-- NumberofCars: string (nullable = true)\n",
      " |-- ZoningCode: string (nullable = true)\n",
      " |-- LotSize: string (nullable = true)\n",
      " |-- YearBuilt: string (nullable = true)\n",
      " |-- CurrentLandValue: string (nullable = true)\n",
      " |-- CurrentImprovementValue: string (nullable = true)\n",
      " |-- Filler8: string (nullable = true)\n",
      " |-- Section: string (nullable = true)\n",
      " |-- Township: string (nullable = true)\n",
      " |-- Range: string (nullable = true)\n",
      " |-- Lot: string (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- TractSubdivision Name: string (nullable = true)\n",
      " |-- MapBook: string (nullable = true)\n",
      " |-- MapPage: string (nullable = true)\n",
      " |-- UnitNum: string (nullable = true)\n",
      " |-- ExpandedLegal1: string (nullable = true)\n",
      " |-- ExpandedLegal2: string (nullable = true)\n",
      " |-- ExpandedLegal3: string (nullable = true)\n",
      " |-- Filler9: string (nullable = true)\n",
      " |-- PIDIRISFRMTD: string (nullable = true)\n",
      " |-- Add/Change/Delete Record: string (nullable = true)\n",
      " |-- DEED SEC CAT CODES: string (nullable = true)\n",
      " |-- mtg sec cat code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3058a25",
   "metadata": {},
   "source": [
    "Let's calculate how many rows and columns there are in total. Note that we will store these values in their own variables because it takes quite a while to calculate the number of rows in our data. If we commit this number to current memory by storing it in a variable, we can easily reuse it later for various purposes without having to recalculate it first. This practice is also known as \"lazy execution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951a0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37487908 134\n"
     ]
    }
   ],
   "source": [
    "numRows = df.count()\n",
    "numCols = len(df.columns)\n",
    "print(numRows,numCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e508e",
   "metadata": {},
   "source": [
    "We can print a list of the column names like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d3ac646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APN',\n",
       " 'FIPS',\n",
       " 'State',\n",
       " 'County',\n",
       " 'BatchDateSeq Number',\n",
       " 'DeedCategory',\n",
       " 'DocumentType',\n",
       " 'RecordingDate',\n",
       " 'DocumentYear',\n",
       " 'DocumentNumber',\n",
       " 'DocumentBook',\n",
       " 'DocumentPage',\n",
       " 'TitleCompanyCode',\n",
       " 'TitleCompanyName',\n",
       " 'AttorneyName',\n",
       " 'AttorneyPhoneNumber',\n",
       " '1stDefendantBorrowerOwnerFirstName',\n",
       " '1stDefendantBorrowerOwnerLastName',\n",
       " '1stDefendantBorrowerOwnerCompanyName',\n",
       " '2ndDefendantBorrowerOwnerFirstName',\n",
       " '2ndDefendantBorrowerOwnerLastName',\n",
       " '2ndDefendantBorrowerOwnerCompanyName',\n",
       " '3rdDefendantBorrowerOwnerFirstName',\n",
       " '3rdDefendantBorrowerOwnerLastName',\n",
       " '3rdDefendantBorrowerOwnerCompanyName',\n",
       " '4thDefendantBorrowerOwnerFirstName',\n",
       " '4thDefendantBorrowerOwnerLastName',\n",
       " '4thDefendantBorrowerOwnerCompanyName',\n",
       " 'DefendantBorrowerOwnerEtAlIndicator',\n",
       " 'Filler1',\n",
       " 'DateofDefault',\n",
       " 'AmountofDefault',\n",
       " 'Filler2',\n",
       " 'FilingDate',\n",
       " 'CourtCaseNumber',\n",
       " 'LisPendensType',\n",
       " 'Plaintiff1/Seller',\n",
       " 'Plaintiff2/Seller',\n",
       " 'FinalJudgmentAmount',\n",
       " 'Filler3',\n",
       " 'AuctionDate',\n",
       " 'AuctionTime',\n",
       " 'StreetAddressofAuctionCall',\n",
       " 'CityofAuctionCall',\n",
       " 'StateofAuctionCall',\n",
       " 'OpeningBid',\n",
       " 'Filler4',\n",
       " 'SalesPrice',\n",
       " 'SitusAddressIndicator1',\n",
       " 'SitusHouseNumberPrefix1',\n",
       " 'SitusHouseNumber1',\n",
       " 'SitusHouseNumberSuffix1',\n",
       " 'SitusStreetName1',\n",
       " 'SitusMode1',\n",
       " 'SitusDirection1',\n",
       " 'SitusQuadrant1',\n",
       " 'ApartmentUnit1',\n",
       " 'PropertyCity1',\n",
       " 'PropertyState1',\n",
       " 'PropertyAddressZipCode1',\n",
       " 'CarrierCode1',\n",
       " 'FullSiteAddressUnparsed1',\n",
       " 'LenderBeneficiaryFirstName',\n",
       " 'LenderBeneficiaryLastName',\n",
       " 'LenderBeneficiaryCompanyName',\n",
       " 'LenderBeneficiaryMailingAddress',\n",
       " 'LenderBeneficiaryCity',\n",
       " 'LenderBeneficiaryState',\n",
       " 'LenderBeneficiaryZip',\n",
       " 'LenderPhone',\n",
       " 'Filler5',\n",
       " 'TrusteeName',\n",
       " 'TrusteeMailing Address',\n",
       " 'TrusteeCity',\n",
       " 'TrusteeState',\n",
       " 'TrusteeZip',\n",
       " 'TrusteePhone',\n",
       " 'TrusteesSaleNumber',\n",
       " 'Filler6',\n",
       " 'OriginalLoanDate',\n",
       " 'OriginalLoanRecordingDate',\n",
       " 'OriginalLoanAmount',\n",
       " 'OriginalDocumentNumber',\n",
       " 'OriginalRecordingBook',\n",
       " 'OriginalRecordingPage',\n",
       " 'Filler7',\n",
       " 'ParcelNumberParcel ID',\n",
       " 'ParcelNumberUnformattedID',\n",
       " 'LastFullSaleTransferDate',\n",
       " 'TransferValue',\n",
       " 'Mailing AddressIndicator2',\n",
       " 'Mailing HouseNumberPrefix2',\n",
       " 'MailingHouseNumber2',\n",
       " 'MailingHouseNumberSuffix2',\n",
       " 'MailingStreetName2',\n",
       " 'MailingMode2',\n",
       " 'MailingDirection2',\n",
       " 'MailingQuadrant2',\n",
       " 'Mailing ApartmentUnit2',\n",
       " 'Mailing PropertyCity2',\n",
       " 'MailingPropertyState2',\n",
       " 'Mailing PropertyAddressZipCode2',\n",
       " 'Mailing CarrierCode2',\n",
       " 'FullSiteAddressUnparsed2',\n",
       " 'PropertyIndicator',\n",
       " 'UseCode',\n",
       " 'NumberofUnits',\n",
       " 'LivingAreaSquareFeet',\n",
       " 'NumberofBedrooms',\n",
       " 'NumberofBathrooms',\n",
       " 'NumberofCars',\n",
       " 'ZoningCode',\n",
       " 'LotSize',\n",
       " 'YearBuilt',\n",
       " 'CurrentLandValue',\n",
       " 'CurrentImprovementValue',\n",
       " 'Filler8',\n",
       " 'Section',\n",
       " 'Township',\n",
       " 'Range',\n",
       " 'Lot',\n",
       " 'Block',\n",
       " 'TractSubdivision Name',\n",
       " 'MapBook',\n",
       " 'MapPage',\n",
       " 'UnitNum',\n",
       " 'ExpandedLegal1',\n",
       " 'ExpandedLegal2',\n",
       " 'ExpandedLegal3',\n",
       " 'Filler9',\n",
       " 'PIDIRISFRMTD',\n",
       " 'Add/Change/Delete Record',\n",
       " 'DEED SEC CAT CODES',\n",
       " 'mtg sec cat code']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a39e3",
   "metadata": {},
   "source": [
    "## Inspecting variable distributions\n",
    "\n",
    "We can use the ```describe``` method to calculate summary statistics for select columns (variables). Summary statistics can only be calculated for columns of type ```integer``` or ```double```. Currently, all of our variables are stored as type ```string```. We can convert a column to numeric values by casting it to type ```integer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c716931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MORTGAGE AMOUNT', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = df.withColumn(\"MORTGAGE AMOUNT\", df[\"MORTGAGE AMOUNT\"].cast(IntegerType()))\n",
    "df.select([\"MORTGAGE AMOUNT\"]).dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd599a7",
   "metadata": {},
   "source": [
    "Now that the 'mortgage amount' column is numeric, we can calculate basic summary statistics (mean, standard deviation, minimum and maximum value). As you can see, the average mortgage amount in the CoreLogic data is about $800K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc32d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|     MORTGAGE AMOUNT|\n",
      "+-------+--------------------+\n",
      "|  count|           111058818|\n",
      "|   mean|   779943.3151940263|\n",
      "| stddev|1.6808331794028625E7|\n",
      "|    min|                   0|\n",
      "|    max|          2075000000|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['MORTGAGE AMOUNT']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ae195",
   "metadata": {},
   "source": [
    "Let's do the same for the sale amount of a property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00601ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|      SALE AMOUNT|\n",
      "+-------+-----------------+\n",
      "|  count|        213239549|\n",
      "|   mean|402407.0318453403|\n",
      "| stddev|6648148.318833588|\n",
      "|    min|                0|\n",
      "|    max|       2128327250|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"SALE AMOUNT\", df[\"SALE AMOUNT\"].cast(IntegerType()))\n",
    "df.describe(['SALE AMOUNT']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a8f7e0",
   "metadata": {},
   "source": [
    "Interestingly, the average sale amount is about half of the average mortgage amount. But these differences could also be driven by missing information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60ae7d",
   "metadata": {},
   "source": [
    "## Exploring the amount of missing data within and across columns\n",
    "\n",
    "It is important to know for columns of interest how many missing values they contain. Information on data missingness guides decisions on what needs to be done with affected rows. Can rows with missing information simply be dropped? Or, do we need to try to fill in missing values conditional on information in other columns?\n",
    "\n",
    "Let's investigate whether the mortgage column contains any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb536162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+\n",
      "|count(CASE WHEN isnan(MORTGAGE AMOUNT) THEN true END)|\n",
      "+-----------------------------------------------------+\n",
      "|                                                    0|\n",
      "+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan('MORTGAGE AMOUNT'),True))]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3859d3",
   "metadata": {},
   "source": [
    "This column does not contain any missing values encoded as ```NaN```. Let's investigate whether the mortgage column containts any ```null``` values. This is just another way of how missing values could be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e9c3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|count(CASE WHEN (MORTGAGE AMOUNT IS NULL) THEN true END)|\n",
      "+--------------------------------------------------------+\n",
      "|                                               256723662|\n",
      "+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col('MORTGAGE AMOUNT').isNull(),True))]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3ef66",
   "metadata": {},
   "source": [
    "That is quite a lot of missing information. We can calculate the percentage of ```null``` values in the mortgage column to get a better idea of the amount of missingness. To do so, we divide the number of rows with ```null``` values in the mortgage amount column by the total number of rows in the CoreLogic data. We can reuse the ```numRows``` variable here that we calculated earlier (see comment on 'lazy execution' above) to save us some processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f71b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|(count(CASE WHEN (MORTGAGE AMOUNT IS NULL) THEN true END) / 367782480)|\n",
      "+----------------------------------------------------------------------+\n",
      "|                                                    0.6980312439026459|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([(count(when(col('MORTGAGE AMOUNT').isNull(),True))/numRows)]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d0cad",
   "metadata": {},
   "source": [
    "As you can see, almost 70% of mortgage amount information is missing in the deeds dataset.\n",
    "\n",
    "Let's calculate the proportion of records for which mortgage AND sale amount information is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e71e47d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|(count(CASE WHEN ((MORTGAGE AMOUNT IS NULL) AND (SALE AMOUNT IS NULL)) THEN true END) / 367782480)|\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|                                                                               0.40302083720790616|\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([(count(when(col('MORTGAGE AMOUNT').isNull() & col('SALE AMOUNT').isNull(), True))/numRows)]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3f6a8",
   "metadata": {},
   "source": [
    "As we can see, about 40% of records have missing information on both the mortgage and sale amount columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91c22de",
   "metadata": {},
   "source": [
    "## Inspecting categorical variables\n",
    "\n",
    "We might be interested in the range of unique values of categorical variables. Let's print the total number of unique values of states designated by the 'SITUS STATE' column. (According to the Oxford Dictionary, \"situs\" is a legal term that designates \"*the place to which, for purposes of legal jurisdiction or taxation, a property belongs.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00ad8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.select('SITUS STATE').distinct().rdd.map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86990683",
   "metadata": {},
   "source": [
    "The SITUS STATE column contains 53 unique values, which is unexpected. There should be 50 states plus Washington, DC. \n",
    "\n",
    "Let's create a frequency table of the column values to get a better idea of this variable. We will sort the table in descending order of state counts. We will set ```n=53``` in the show() method to print all rows in the table. The two additional values are: (1) ```null``` (the amount of missing values in the SITUS STATE column), and (2) `VI`, which stands for the Virgin Islands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78155564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|SITUS STATE|   count|\n",
      "+-----------+--------+\n",
      "|         CA|52167596|\n",
      "|       null|35371569|\n",
      "|         FL|35281257|\n",
      "|         TX|21674192|\n",
      "|         OH|13347704|\n",
      "|         PA|12254911|\n",
      "|         AZ|12006118|\n",
      "|         IL|11947950|\n",
      "|         GA|11148302|\n",
      "|         NY|10456027|\n",
      "|         NC| 9686809|\n",
      "|         MI| 8638900|\n",
      "|         TN| 8638649|\n",
      "|         CO| 8112459|\n",
      "|         WA| 8104844|\n",
      "|         VA| 7827533|\n",
      "|         NJ| 7320302|\n",
      "|         MD| 6525203|\n",
      "|         MO| 6496410|\n",
      "|         SC| 6311502|\n",
      "|         OK| 5217718|\n",
      "|         MA| 4876134|\n",
      "|         OR| 4767128|\n",
      "|         NV| 4712444|\n",
      "|         IN| 4394358|\n",
      "|         AL| 4379161|\n",
      "|         AR| 4378459|\n",
      "|         MN| 4267855|\n",
      "|         WI| 3864221|\n",
      "|         UT| 3154019|\n",
      "|         KY| 3018343|\n",
      "|         MS| 2978654|\n",
      "|         HI| 2572320|\n",
      "|         CT| 2513164|\n",
      "|         LA| 2497956|\n",
      "|         WV| 2374639|\n",
      "|         KS| 2348984|\n",
      "|         IA| 1982068|\n",
      "|         NM| 1546001|\n",
      "|         NE| 1365483|\n",
      "|         MT| 1289606|\n",
      "|         ID| 1138025|\n",
      "|         DE|  905431|\n",
      "|         ME|  614182|\n",
      "|         NH|  559467|\n",
      "|         RI|  465933|\n",
      "|         DC|  457645|\n",
      "|         AK|  428840|\n",
      "|         ND|  426858|\n",
      "|         VT|  329881|\n",
      "|         SD|  318106|\n",
      "|         WY|  300540|\n",
      "|         VI|   20620|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.groupBy('SITUS STATE').count().sort(desc(\"count\")).show(n=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b8b3c",
   "metadata": {},
   "source": [
    "## Inspecting time variables\n",
    "\n",
    "We might be interested in columns that present information about time/dates.\n",
    "\n",
    "Let's identify the columns that seem to contain date information. As we can see below, dates are stored in the format ```YYYYMMDD``` or ```YYYY-MM-DD```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "523374c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------+----------+-----------+----------------+-------------------------+------------------------+\n",
      "|BatchDateSeq Number|RecordingDate|DateofDefault|FilingDate|AuctionDate|OriginalLoanDate|OriginalLoanRecordingDate|LastFullSaleTransferDate|\n",
      "+-------------------+-------------+-------------+----------+-----------+----------------+-------------------------+------------------------+\n",
      "| 201503180000000002|     20150317|         null|  20150304|   20150507|            null|                     null|              2000-06-02|\n",
      "| 201605050000000001|     20160503|         null|  20160406|   20160526|            null|                     null|                    null|\n",
      "| 193001010000021584|         null|         null|      null|       null|            null|                     null|                    null|\n",
      "| 201506030000000054|     20150521|         null|  20150424|       null|        20070703|                 20070718|              2007-07-03|\n",
      "| 193001010000021600|         null|         null|      null|       null|            null|                     null|              2001-09-27|\n",
      "| 193001010000021601|         null|         null|      null|       null|            null|                     null|                    null|\n",
      "| 200807180000000088|     20080708|         null|  20080703|       null|            null|                     null|              2005-04-20|\n",
      "| 201210260000000087|     20120926|         null|  20120920|       null|            null|                     null|              2006-10-30|\n",
      "| 201508040000000013|     20150720|         null|  20150626|       null|            null|                     null|              2006-04-03|\n",
      "| 201607120000000065|     20160518|         null|      null|       null|            null|                     null|              2006-04-03|\n",
      "| 193001010000021681|         null|         null|      null|       null|            null|                     null|              1997-09-01|\n",
      "| 201110170000000016|     20111004|         null|  20110928|       null|            null|                     null|              2003-07-29|\n",
      "| 201702270000000030|     20170214|         null|  20161110|       null|        20051014|                 20051021|              2003-07-29|\n",
      "| 200402210000000079|     20040113|         null|      null|       null|            null|                     null|                    null|\n",
      "| 201201040000000007|     20111216|         null|  20111216|       null|            null|                     null|              1995-08-01|\n",
      "| 201506190000000023|     20150608|         null|      null|       null|            null|                     null|              1985-04-15|\n",
      "| 200704110000000028|     20070326|         null|      null|       null|            null|                     null|                    null|\n",
      "| 201211280000000004|     20121023|         null|      null|       null|            null|                     null|                    null|\n",
      "| 201609130000000016|     20160629|         null|      null|       null|            null|                     null|              2004-07-16|\n",
      "| 193001010000021782|         null|         null|      null|       null|            null|                     null|              2001-10-01|\n",
      "+-------------------+-------------+-------------+----------+-----------+----------------+-------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([s for s in df.columns if 'Date' in s]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28412b98",
   "metadata": {},
   "source": [
    "Let's work with the ```RecordingDate``` column to identify the time period covered in the CoreLogic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6bd9540",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-72828b87f94e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RecordingDate'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RecordingDate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"YYYYMMDD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recording_date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_date'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.select('RecordingDate'.to_date('RecordingDate', \"YYYYMMDD\").alias(\"recording_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d157f56",
   "metadata": {},
   "source": [
    "## Filtering the CoreLogic data to create a subset of observations\n",
    "\n",
    "Suppose we only needed a subset of the CoreLogic data for research purposes, i.e., we were interested in property information in a select location. \n",
    "\n",
    "Here, our goal is to filter the CoreLogic data to create a subset that only contains rows relating to Detroit, Michigan. \n",
    "\n",
    "To start, let's print a select few rows and columns of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a22b0d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+--------------+\n",
      "| FIPS|SITUS CITY|SITUS STATE|SITUS ZIP CODE|\n",
      "+-----+----------+-----------+--------------+\n",
      "|12099|      null|       null|          null|\n",
      "|17097|      null|       null|          null|\n",
      "|97200|      null|       null|          null|\n",
      "|97200|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "|17097|      null|       null|          null|\n",
      "|97199|      null|       null|          null|\n",
      "+-----+----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('FIPS', 'SITUS CITY', 'SITUS STATE', 'SITUS ZIP CODE').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86b461",
   "metadata": {},
   "source": [
    "Notice how there are many rows containing columns with ```null``` values. We will remove rows that have ```null``` values across all columns of interest because it would be hard to deduce the location without any of this information (FIPS code, property city, state and zip code) present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df7ea2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"FIPS\",\"SITUS CITY\", \"SITUS STATE\", \"SITUS ZIP CODE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a1f3f",
   "metadata": {},
   "source": [
    "Now let's see how many rows we have left out of the original 367,782,480:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef93115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311359183"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433cde04",
   "metadata": {},
   "source": [
    "Let's assume we wanted to filter the CoreLogic data for Wayne county, MI, where the city of Detroit is located. The FIPS code for Wayne county is '26163' ([source](https://mi.postcodebase.com/county/26163))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30f4902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WayneCsubset = df.filter(df.FIPS==26163)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2467d87",
   "metadata": {},
   "source": [
    "Let's print a few rows and columns of the subset. As you can see, we are only dealing with Wayne county now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6648b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+--------------+\n",
      "| FIPS|SITUS CITY|SITUS STATE|SITUS ZIP CODE|\n",
      "+-----+----------+-----------+--------------+\n",
      "|26163|   DETROIT|         MI|     482013148|\n",
      "|26163|   DETROIT|         MI|     482013148|\n",
      "|26163|   DETROIT|         MI|     482013148|\n",
      "|26163|   DETROIT|         MI|     482013148|\n",
      "|26163|   DETROIT|         MI|     482012463|\n",
      "|26163|   DETROIT|         MI|         48202|\n",
      "|26163|   DETROIT|         MI|         48202|\n",
      "|26163|   DETROIT|         MI|         48202|\n",
      "|26163|   DETROIT|         MI|     482022828|\n",
      "|26163|   DETROIT|         MI|     482022828|\n",
      "|26163|   DETROIT|         MI|     482022828|\n",
      "|26163|   DETROIT|         MI|     482022828|\n",
      "|26163|   DETROIT|         MI|     482022828|\n",
      "|26163|   DETROIT|         MI|         48202|\n",
      "|26163|   DETROIT|         MI|     482021302|\n",
      "|26163|   DETROIT|         MI|     482021368|\n",
      "|26163|   DETROIT|         MI|     482021368|\n",
      "|26163|   DETROIT|         MI|     482021368|\n",
      "|26163|   DETROIT|         MI|     482021368|\n",
      "|26163|   DETROIT|         MI|     482021368|\n",
      "+-----+----------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WayneCsubset.select('FIPS', 'SITUS CITY', 'SITUS STATE', 'SITUS ZIP CODE').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb7ce7",
   "metadata": {},
   "source": [
    "How many rows are in the Wayne county subset? We can use the count() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01514600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132959"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WayneCsubset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a766ab9",
   "metadata": {},
   "source": [
    "There are many different cities within Wayne county. Let's get a list of all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f61109f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SITUS CITY='LINCOLN PARK'),\n",
       " Row(SITUS CITY='MELVINDALE'),\n",
       " Row(SITUS CITY='NORTHVILLE'),\n",
       " Row(SITUS CITY='TRENTON'),\n",
       " Row(SITUS CITY='YPSILANTI'),\n",
       " Row(SITUS CITY='BELLEVILLE'),\n",
       " Row(SITUS CITY='OAK PARK'),\n",
       " Row(SITUS CITY='RIVERVIEW'),\n",
       " Row(SITUS CITY='GROSSE POINTE WOODS'),\n",
       " Row(SITUS CITY='FLAT ROCK'),\n",
       " Row(SITUS CITY='GROSSE ILE'),\n",
       " Row(SITUS CITY='GROSSE POINTE PARK'),\n",
       " Row(SITUS CITY='WARREN'),\n",
       " Row(SITUS CITY='GIBRALTAR'),\n",
       " Row(SITUS CITY='GROSSE POINTE'),\n",
       " Row(SITUS CITY='MONROE'),\n",
       " Row(SITUS CITY='WYANDOTTE'),\n",
       " Row(SITUS CITY='HIGHLAND'),\n",
       " Row(SITUS CITY='WOODHAVEN'),\n",
       " Row(SITUS CITY='INKSTER'),\n",
       " Row(SITUS CITY='SOUTHGATE'),\n",
       " Row(SITUS CITY='SUMPTER TWP'),\n",
       " Row(SITUS CITY='GARDEN CITY'),\n",
       " Row(SITUS CITY='HIGHLAND PARK'),\n",
       " Row(SITUS CITY='DETROIT'),\n",
       " Row(SITUS CITY='FERNDALE'),\n",
       " Row(SITUS CITY='CHELSEA'),\n",
       " Row(SITUS CITY='BROWNSTOWN TWP'),\n",
       " Row(SITUS CITY='NEW HUDSON'),\n",
       " Row(SITUS CITY='LANSING'),\n",
       " Row(SITUS CITY='DEARBORN HEIGHTS'),\n",
       " Row(SITUS CITY='CANTON'),\n",
       " Row(SITUS CITY='DEARBORN'),\n",
       " Row(SITUS CITY='GROSSE POINTE SHORES'),\n",
       " Row(SITUS CITY='ROMULUS'),\n",
       " Row(SITUS CITY='BINGHAM FARMS'),\n",
       " Row(SITUS CITY='LIVONIA'),\n",
       " Row(SITUS CITY='RIVER ROUGE'),\n",
       " Row(SITUS CITY='VAN BUREN TWP'),\n",
       " Row(SITUS CITY='NEW BOSTON'),\n",
       " Row(SITUS CITY='WESTLAND'),\n",
       " Row(SITUS CITY='GROSSE POINTE FARMS'),\n",
       " Row(SITUS CITY='PLYMOUTH'),\n",
       " Row(SITUS CITY='BROWNSTOWN'),\n",
       " Row(SITUS CITY='REDFORD'),\n",
       " Row(SITUS CITY='WAYNE'),\n",
       " Row(SITUS CITY='HARPER WOODS'),\n",
       " Row(SITUS CITY='TAYLOR'),\n",
       " Row(SITUS CITY='CARLETON'),\n",
       " Row(SITUS CITY='ALLEN PARK'),\n",
       " Row(SITUS CITY='SOUTHFIELD'),\n",
       " Row(SITUS CITY='ECORSE'),\n",
       " Row(SITUS CITY='ROCKWOOD'),\n",
       " Row(SITUS CITY='BROWNSTOWN TOWNSHIP'),\n",
       " Row(SITUS CITY='HAMTRAMCK')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WayneCsubset.select('SITUS CITY').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842db83c",
   "metadata": {},
   "source": [
    "Let's create a subset for Detroit only. We will filter using the 'SITUS CITY' variable. This variables designates the city associated with the property address. \n",
    "\n",
    "Note that this is a straightforward filtering method if using the Wayne county subset we created previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd6c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroit = WayneCsubset.filter(WayneCsubset[\"SITUS CITY\"]==\"DETROIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a19a40",
   "metadata": {},
   "source": [
    "If you wanted to create a Detroit subset from the entire dataset, we suggest filtering on 'SITUS CITY' and 'SITUS STATE'. When using the entire dataframe, filtering on 'SITUS CITY' alone could run into city name ambiguity across states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c54340b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroit2 = df.filter((df[\"SITUS CITY\"]==\"DETROIT\") & (df[\"SITUS STATE\"]==\"MI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294452ef",
   "metadata": {},
   "source": [
    "How many rows are in the Detroit subset? Depending on the size of the dataframe, count() can take a bit of time to calculate. Therefore, we will save the value to a variable in case we need to use it later on. This can save us some processing time (cf. comment on \"lazy evaluation\" above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b4aba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129407"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detroitCount = detroit.count()\n",
    "detroitCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed95cf4",
   "metadata": {},
   "source": [
    "Do the two filtering methods yield the same number of rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf449eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1129438"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detroit2Count = detroit2.count()\n",
    "detroit2Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b9ff6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detroit2Count - detroitCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb79a17",
   "metadata": {},
   "source": [
    "The second subset contains slightly more records. This means that we get two different results depending on whether we filter by (SITUS CITY == \"DETROIT\" AND SITUS STATE == \"MI\") versus (FIPS == \"26163\" AND SITUS CITY == \"DETROIT\"). \n",
    "\n",
    "Let's investigate the Detroit subset with more records (```detroit2```). We can tabulate the FIPS codes that have city==\"DETROIT\" and state==\"MI\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f3ce289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+-------+\n",
      "| FIPS|SITUS CITY|SITUS STATE|  count|\n",
      "+-----+----------+-----------+-------+\n",
      "|26147|   DETROIT|         MI|      6|\n",
      "|26061|   DETROIT|         MI|      2|\n",
      "|26035|   DETROIT|         MI|      2|\n",
      "|26161|   DETROIT|         MI|      1|\n",
      "|26163|   DETROIT|         MI|1129407|\n",
      "|48039|   DETROIT|         MI|      2|\n",
      "|26069|   DETROIT|         MI|      2|\n",
      "|26071|   DETROIT|         MI|      5|\n",
      "|26099|   DETROIT|         MI|      2|\n",
      "|26115|   DETROIT|         MI|      9|\n",
      "+-----+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detroit2.groupBy('FIPS', 'SITUS CITY', 'SITUS STATE').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46099de2",
   "metadata": {},
   "source": [
    "As you can see, there are multiple FIPS codes associated with Detroit, MI, in this data subset when there should really only be one FIPS code for Detroit in Wayne County, Michigan.  For example, FIPS code '26115' (9 occurences) is for Monroe County which does not contain the city of Detroit. \n",
    "\n",
    "For each of the additional FIPS codes identified above, we would have to do some further research to determine whether these FIPS codes constitute data entry errors that need to be corrected or whether they identify properties that are not actually located in Detroit. To do this, we would have to review each FIPS code and also inspect the addresses associated with each property to determine where exactly a property is legally located. This work is beyond the purposes of the current analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0e711",
   "metadata": {},
   "source": [
    "## Saving a subset of the CoreLogic data to a spreadsheet\n",
    "\n",
    "Let's assume the detroit2 dataframe looks like what we are interested in and we now want to write this Detroit subset to a new csv file in an ```output/``` folder in our ```home/[uniqname]/``` directory. (Note: You probably need to change this path, depending on where you store this notebook.) \n",
    "\n",
    "By default, Spark writes big data into multiple files to optimize computation times during import and export. This is recommended practice, especially if the data is big, i.e., if it has many rows and/or columns.\n",
    "\n",
    "The below command writes 100 partitions of the Detroit data (i.e., 100 csv files) to the designated folder. \n",
    "\n",
    "_Note_: If you saved data to this path before, you will need to add the argument ```overwrite=True``` to the function call like so ```detroit2.write.csv(\"../../output/corelogic_data_deeds_Detroit_partitioned\", overwrite=True)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ab02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroit2.write.csv(\"../../output/corelogic_data_deeds_Detroit_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bec64",
   "metadata": {},
   "source": [
    "There is also a way to force the data into one single file. Sometimes, a single file is needed to continue processing the data with other research software. Saving the data into one file is a little risky and might not always work, so caution is advised. \n",
    "\n",
    "The following command combines the data into one core (Caution: it is very slow). If the data does not fit, it will throw an error. _Note_: If you saved data to this path before, you will need to add the argument ```overwrite=True``` to the function call. The overwrite argument would overwrite the previously created ```output/``` directory.\n",
    "\n",
    "Note: You would only need to run this, if you indeed need only one single data file. Many research programs allow you to read in and combine multiple data files into one object. See for example the [glob method in Python](https://docs.python.org/3/library/glob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d0a9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroit2.coalesce(1).write.csv(\"../../output/corelogic_data_deeds_Detroit_singlefile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c58568",
   "metadata": {},
   "source": [
    "## Drawing a data sample for research and development purposes\n",
    "Let's say we want to draw and save just a sample of the Detroit data (n~1000) that is small enough to use for writing and testing code for analyzing the Detroit subset. We can use PySpark's DataFrame.sample() method. However, we have to specify a fraction of the dataframe size rather than explicitly the size of the sample, n. Also, we are not guaranteed to get EXACTLY the fraction that we specify, due to the way the sample() function is implemented. Let's see how that works.\n",
    "\n",
    "Since we want about 1000 rows in our sample, we can divide 1000 by detroit2Count to see what fraction that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92f403df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008853960996531018"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleFraction = 1000 / detroit2Count\n",
    "sampleFraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9352aac",
   "metadata": {},
   "source": [
    "Now, let's sample the Detroit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b59e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroitSample = detroit2.sample(fraction=sampleFraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca12fd8",
   "metadata": {},
   "source": [
    "How many records do we have in our sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc78e8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detroitSample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e58204",
   "metadata": {},
   "source": [
    "Now that we have created a small sample of our Detroit records for research and development purposes, we can write the results to a spreadsheet (note that using coalesce(1) can still be quite expensive. Use only if absolutely necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c18402",
   "metadata": {},
   "outputs": [],
   "source": [
    "detroitSample.write.csv(\"../../output/corelogic_data_deeds_Detroit_sampled_1031_singlefile\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
